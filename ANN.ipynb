{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "#from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15998, 99) (4002, 99)\n"
     ]
    }
   ],
   "source": [
    "tr = '../Data/train.csv'\n",
    "ts = '../Data/test.csv'\n",
    "train_set = pd.read_csv(tr)\n",
    "test_set  = pd.read_csv(ts)\n",
    "\n",
    "# print ('Training set')\n",
    "# print (train_set.head())\n",
    "# print ('\\nTest set')\n",
    "# print (test_set.head())\n",
    "# print ('\\nOriginal DataFrame')\n",
    "# print (data.head())\n",
    "\n",
    "print(train_set.shape,test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_set.iloc[:,:93]\n",
    "Y_train = train_set.iloc[:,93:]\n",
    "\n",
    "X_test  = test_set.iloc[:,:93]\n",
    "Y_test  = test_set.iloc[:,93:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (93, 15998)\n",
      "Y_train shape: (6, 15998)\n",
      "X_test shape: (93, 4002)\n",
      "Y_test shape: (6, 4002)\n",
      "                                              0         1         2     \\\n",
      "Acedamic percentage in Operating Systems  1.091866  0.396841  0.694709   \n",
      "percentage in Algorithms                 -1.683226  0.099190 -0.296902   \n",
      "Percentage in Programming Concepts        0.683037 -1.388474 -0.993901   \n",
      "Percentage in Software Engineering       -0.011203 -0.011203 -0.011203   \n",
      "Percentage in Computer Networks           1.601635 -0.195059  0.004573   \n",
      "\n",
      "                                              3         4         5     \\\n",
      "Acedamic percentage in Operating Systems  0.098973 -1.688235 -0.993210   \n",
      "percentage in Algorithms                 -0.296902  1.584537  1.584537   \n",
      "Percentage in Programming Concepts       -1.585761  0.091176 -0.895257   \n",
      "Percentage in Software Engineering        1.475688 -1.002463 -0.605959   \n",
      "Percentage in Computer Networks           0.603472  1.202370  0.603472   \n",
      "\n",
      "                                              6         7         8     \\\n",
      "Acedamic percentage in Operating Systems  1.489023 -0.893920  0.893287   \n",
      "percentage in Algorithms                  1.683560  0.099190  0.594306   \n",
      "Percentage in Programming Concepts        0.387106 -0.204754  0.880323   \n",
      "Percentage in Software Engineering       -0.011203  1.376562  0.385302   \n",
      "Percentage in Computer Networks           0.304023  1.002737 -1.392856   \n",
      "\n",
      "                                              9       ...         3992  \\\n",
      "Acedamic percentage in Operating Systems  0.496130    ...    -0.596052   \n",
      "percentage in Algorithms                  1.188444    ...     0.594306   \n",
      "Percentage in Programming Concepts        1.176254    ...     0.485750   \n",
      "Percentage in Software Engineering       -0.506833    ...     1.673940   \n",
      "Percentage in Computer Networks           0.603472    ...     1.402003   \n",
      "\n",
      "                                              3993      3994      3995  \\\n",
      "Acedamic percentage in Operating Systems -0.000317  0.198262 -0.993210   \n",
      "percentage in Algorithms                  1.386491 -1.485179 -0.296902   \n",
      "Percentage in Programming Concepts       -0.303397  0.880323 -1.191188   \n",
      "Percentage in Software Engineering       -1.498094  0.980058 -1.498094   \n",
      "Percentage in Computer Networks          -1.093407  1.601635  0.403839   \n",
      "\n",
      "                                              3996      3997      3998  \\\n",
      "Acedamic percentage in Operating Systems  1.191155  0.595419  0.297551   \n",
      "percentage in Algorithms                  0.990398 -0.990064 -0.792018   \n",
      "Percentage in Programming Concepts       -0.106110 -1.487118  0.584393   \n",
      "Percentage in Software Engineering       -1.498094 -1.398967  0.980058   \n",
      "Percentage in Computer Networks           0.503655  0.403839  0.503655   \n",
      "\n",
      "                                              3999      4000      4001  \n",
      "Acedamic percentage in Operating Systems  0.595419 -0.893920 -0.397474  \n",
      "percentage in Algorithms                 -0.692995  0.990398  0.000167  \n",
      "Percentage in Programming Concepts        0.288463  1.373540 -0.303397  \n",
      "Percentage in Software Engineering        0.980058  1.079184  0.682680  \n",
      "Percentage in Computer Networks          -1.293039 -1.093407 -0.694141  \n",
      "\n",
      "[5 rows x 4002 columns]\n"
     ]
    }
   ],
   "source": [
    "# Explore your dataset \n",
    "# X_train = X_train.T\n",
    "# Y_train = Y_train.T\n",
    "# X_test  = X_test.T\n",
    "# Y_test  = Y_test.T\n",
    "\n",
    "m_train = X_train.shape[1] #no of train samples\n",
    "n       = X_train.shape[0] #no of train features\n",
    "m_test  = Y_test.shape[1] #no of test samples\n",
    "\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))\n",
    "print(X_test.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalising the marks columns (1-9)\n",
    "# myu = [77.02508004268944,  76.95090715048026,  77.03868729989328,  77.13046958377801, 76.94223585912486,  77.0510272145144,   76.92062433297758,  76.91355389541089, 7.981456776947706,  5.005803094983992,  2.974919957310566,  5.009271611526147, 5.001867662753469]\n",
    "# sig = [10.069482344113009, 10.100838612474481, 10.155240754077084, 10.10313965184933, 10.008561395008378, 10.105803974768442, 10.159253374201388, 10.09967655273072, 2.5876773324068223, 2.5826669245403986, 2.0067022963210617, 2.576243318124351, 2.577799489969915]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: create_placeholders\n",
    "#print(X_train.head())\n",
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, no of features (93)\n",
    "    n_y -- scalar, number of classes (34)\n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
    "    \n",
    "    Tips:\n",
    "    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.\n",
    "      In fact, the number of examples during test/train is different.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    X = tf.placeholder(dtype = \"float32\" , shape = (n_x,None) , name=\"X\")\n",
    "    Y = tf.placeholder(dtype = \"float32\" , shape = (n_y,None) , name=\"Y\")\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = Tensor(\"X:0\", shape=(12288, ?), dtype=float32)\n",
      "Y = Tensor(\"Y:0\", shape=(6, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Tesing\n",
    "X, Y = create_placeholders(12288, 6)\n",
    "print (\"X = \" + str(X))\n",
    "print (\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [50, 93]\n",
    "                        b1 : [50, 1]\n",
    "                        W2 : [45, 50]\n",
    "                        b2 : [45, 1]\n",
    "                        W3 : [40, 45]\n",
    "                        b3 : [40, 1]\n",
    "                        W4 : [38, 40]\n",
    "                        b4 : [38, 1]\n",
    "                        W5 : [36, 38]\n",
    "                        b5 : [36, 1]\n",
    "                        W6 : [34, 36]\n",
    "                        b6 : [34, 1]\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)                   # so that your \"random\" numbers match ours\n",
    "        \n",
    "    ### START CODE HERE ### (approx. 6 lines of code)\n",
    "    W1 = tf.get_variable(\"W1\", [60, 93], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b1 = tf.get_variable(\"b1\", [60, 1], initializer = tf.zeros_initializer())\n",
    "    W2 = tf.get_variable(\"W2\", [60, 60], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b2 = tf.get_variable(\"b2\", [60, 1], initializer = tf.zeros_initializer())\n",
    "    W3 = tf.get_variable(\"W3\", [60, 60], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b3 = tf.get_variable(\"b3\", [60, 1], initializer = tf.zeros_initializer())\n",
    "    W4 = tf.get_variable(\"W4\", [60, 60], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b4 = tf.get_variable(\"b4\", [60, 1], initializer = tf.zeros_initializer())\n",
    "    W5 = tf.get_variable(\"W5\", [60, 60], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b5 = tf.get_variable(\"b5\", [60, 1], initializer = tf.zeros_initializer())\n",
    "    W6 = tf.get_variable(\"W6\", [60, 60], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b6 = tf.get_variable(\"b6\", [60, 1], initializer = tf.zeros_initializer())\n",
    "    W7 = tf.get_variable(\"W7\", [60, 60], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b7 = tf.get_variable(\"b7\", [60, 1], initializer = tf.zeros_initializer())\n",
    "    W8 = tf.get_variable(\"W8\", [60, 60], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b8 = tf.get_variable(\"b8\", [60, 1], initializer = tf.zeros_initializer())\n",
    "    W9 = tf.get_variable(\"W9\", [35, 60], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b9 = tf.get_variable(\"b9\", [35, 1], initializer = tf.zeros_initializer())\n",
    "    W10 = tf.get_variable(\"W10\", [6, 35], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b10 = tf.get_variable(\"b10\", [6, 1], initializer = tf.zeros_initializer())\n",
    "#     W11 = tf.get_variable(\"W11\", [35, 35], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b11 = tf.get_variable(\"b11\", [35, 1], initializer = tf.zeros_initializer())\n",
    "#     W12 = tf.get_variable(\"W12\", [35, 35], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b12 = tf.get_variable(\"b12\", [35, 1], initializer = tf.zeros_initializer())\n",
    "#     W13 = tf.get_variable(\"W13\", [35, 35], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b13 = tf.get_variable(\"b13\", [35, 1], initializer = tf.zeros_initializer())\n",
    "#     W14 = tf.get_variable(\"W14\", [35, 35], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b14 = tf.get_variable(\"b14\", [35, 1], initializer = tf.zeros_initializer())\n",
    "#     W15 = tf.get_variable(\"W15\", [35, 35], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b15 = tf.get_variable(\"b15\", [35, 1], initializer = tf.zeros_initializer())\n",
    "#     W16 = tf.get_variable(\"W16\", [35, 35], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b16 = tf.get_variable(\"b16\", [35, 1], initializer = tf.zeros_initializer())\n",
    "#     W17 = tf.get_variable(\"W17\", [35, 35], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b17 = tf.get_variable(\"b17\", [35, 1], initializer = tf.zeros_initializer())\n",
    "#     W18 = tf.get_variable(\"W18\", [35, 35], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b18 = tf.get_variable(\"b18\", [35, 1], initializer = tf.zeros_initializer())\n",
    "#     W19 = tf.get_variable(\"W19\", [35, 35], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b19 = tf.get_variable(\"b19\", [35, 1], initializer = tf.zeros_initializer())\n",
    "#     W20 = tf.get_variable(\"W20\", [35, 35], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b20 = tf.get_variable(\"b20\", [8, 1], initializer = tf.zeros_initializer())\n",
    "#     W21 = tf.get_variable(\"W21\", [40, 40], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b21 = tf.get_variable(\"b21\", [40, 1], initializer = tf.zeros_initializer())\n",
    "#     W22 = tf.get_variable(\"W22\", [40, 40], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b22 = tf.get_variable(\"b22\", [40, 1], initializer = tf.zeros_initializer())\n",
    "#     W23 = tf.get_variable(\"W23\", [40, 40], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b23 = tf.get_variable(\"b23\", [40, 1], initializer = tf.zeros_initializer())\n",
    "#     W24 = tf.get_variable(\"W24\", [40, 40], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b24 = tf.get_variable(\"b24\", [40, 1], initializer = tf.zeros_initializer())\n",
    "#     W25 = tf.get_variable(\"W25\", [40, 40], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b25 = tf.get_variable(\"b25\", [40, 1], initializer = tf.zeros_initializer())\n",
    "#     W26 = tf.get_variable(\"W26\", [40, 40], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b26 = tf.get_variable(\"b26\", [40, 1], initializer = tf.zeros_initializer())\n",
    "#     W27 = tf.get_variable(\"W27\", [40, 40], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b27 = tf.get_variable(\"b27\", [40, 1], initializer = tf.zeros_initializer())\n",
    "#     W28 = tf.get_variable(\"W28\", [40, 40], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b28 = tf.get_variable(\"b28\", [40, 1], initializer = tf.zeros_initializer())\n",
    "#     W29 = tf.get_variable(\"W29\", [38, 40], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b29 = tf.get_variable(\"b29\", [38, 1], initializer = tf.zeros_initializer())\n",
    "#     W30 = tf.get_variable(\"W30\", [36, 38], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b30 = tf.get_variable(\"b30\", [36, 1], initializer = tf.zeros_initializer())\n",
    "#     W31 = tf.get_variable(\"W31\", [34, 36], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "#     b31 = tf.get_variable(\"b31\", [34, 1], initializer = tf.zeros_initializer())\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    parameters = {\"W1\": W1,\"b1\": b1,\"W2\": W2,\"b2\": b2,\"W3\": W3,\"b3\": b3,\"W4\": W4,\"b4\": b4,\n",
    "                  \"W5\": W5,\"b5\": b5,\n",
    "                  \"W6\": W6,\"b6\": b6,\n",
    "                  \"W7\": W7,\"b7\": b7,\"W8\": W8,\"b8\": b8,\n",
    "                  \"W9\": W9,\"b9\": b9,\"W10\": W10,\"b10\": b10,\n",
    "#                   \"W11\": W11,\"b11\": b11,\"W12\": W12,\"b12\": b12,\n",
    "#                   \"W13\":W13,\"b13\":b13,\"W14\": W14,\"b14\": b14,\"W15\": W15,\"b15\": b15,\n",
    "#                   \"W16\": W16,\"b16\": b16,\n",
    "#                   \"W17\":W17,\"b17\":b17,\"W18\": W18,\"b18\": b18,\"W19\": W19,\"b19\": b19,\"W20\": W20,\"b20\": b20,\n",
    "#                   \"W21\":W21,\"b21\":b21,\"W22\":W22,\"b22\":b22,\"W23\":W23,\"b23\":b23,\"W24\":W24,\"b24\":b24,\n",
    "#                   \"W25\":W25,\"b25\":b25,\"W26\":W26,\"b26\":b26,\"W27\":W27,\"b27\":b27,\"W28\":W28,\"b28\":b28,\n",
    "#                   \"W29\":W29,\"b29\":b29,\"W30\":W30,\"b30\":b30,\"W31\":W31,\"b31\":b31\n",
    "#                  \n",
    "                 }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'W19'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-b986eedb1a31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"W19 = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W19\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"b19 = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b19\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"W20 = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W20\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'W19'"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    parameters = initialize_parameters()\n",
    "    print(\"W19 = \" + str(parameters[\"W19\"]))\n",
    "    print(\"b19 = \" + str(parameters[\"b19\"]))\n",
    "    print(\"W20 = \" + str(parameters[\"W20\"]))\n",
    "    print(\"b20 = \" + str(parameters[\"b20\"]))\n",
    "#     print(\"W3 = \" + str(parameters[\"W3\"]))\n",
    "#     print(\"b3 = \" + str(parameters[\"b3\"]))\n",
    "#     print(\"W4 = \" + str(parameters[\"W4\"]))\n",
    "#     print(\"b4 = \" + str(parameters[\"b4\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    W4 = parameters['W4']\n",
    "    b4 = parameters['b4']\n",
    "    W5 = parameters['W5']\n",
    "    b5 = parameters['b5']\n",
    "    W6 = parameters['W6']\n",
    "    b6 = parameters['b6']\n",
    "    W7 = parameters['W7']\n",
    "    b7 = parameters['b7']\n",
    "    W8 = parameters['W8']\n",
    "    b8 = parameters['b8']\n",
    "    W9 = parameters['W9']\n",
    "    b9 = parameters['b9']\n",
    "    W10 = parameters['W10']\n",
    "    b10 = parameters['b10']\n",
    "#     W11 = parameters['W11']\n",
    "#     b11 = parameters['b11']\n",
    "#     W12 = parameters['W12']\n",
    "#     b12 = parameters['b12']\n",
    "#     W13 = parameters['W13']\n",
    "#     b13 = parameters['b13']\n",
    "#     W14 = parameters['W14']\n",
    "#     b14 = parameters['b14']\n",
    "#     W15 = parameters['W15']\n",
    "#     b15 = parameters['b15']\n",
    "#     W16 = parameters['W16']\n",
    "#     b16 = parameters['b16']\n",
    "#     W17 = parameters['W17']\n",
    "#     b17 = parameters['b17']\n",
    "#     W18 = parameters['W18']\n",
    "#     b18 = parameters['b18']\n",
    "#     W19 = parameters['W19']\n",
    "#     b19 = parameters['b19']\n",
    "#     W20 = parameters['W20']\n",
    "#     b20 = parameters['b20']\n",
    "#     W21 = parameters['W21']\n",
    "#     b21 = parameters['b21']\n",
    "#     W22 = parameters['W22']\n",
    "#     b22 = parameters['b22']\n",
    "#     W23 = parameters['W23']\n",
    "#     b23 = parameters['b23']\n",
    "#     W24 = parameters['W24']\n",
    "#     b24 = parameters['b24']\n",
    "#     W25 = parameters['W25']\n",
    "#     b25 = parameters['b25']\n",
    "#     W26 = parameters['W26']\n",
    "#     b26 = parameters['b26']\n",
    "#     W27 = parameters['W27']\n",
    "#     b27 = parameters['b27']\n",
    "#     W28 = parameters['W28']\n",
    "#     b28 = parameters['b28']\n",
    "#     W29 = parameters['W29']\n",
    "#     b29 = parameters['b29']\n",
    "#     W30 = parameters['W30']\n",
    "#     b30 = parameters['b30']\n",
    "#     W31 = parameters['W31']\n",
    "#     b31 = parameters['b31']\n",
    "    \n",
    "    \n",
    "    ### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:\n",
    "    keep_prob = 0.75\n",
    "    Z1 = tf.add(tf.matmul(tf.nn.dropout(W1, keep_prob),X),b1)                        # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2,A1),b2)                       # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(tf.nn.dropout(W3, keep_prob),A2),b3)                       # Z3 = np.dot(W3,a2) + b3\n",
    "    A3 = tf.nn.relu(Z3)                                  # A3 = relu(Z3)\n",
    "    Z4 = tf.add(tf.matmul(W4,A3),b4)                       # Z4 = np.dot(W4,a3) + b4\n",
    "    A4 = tf.nn.relu(Z4)                                    # A4 = relu(Z4)\n",
    "    Z5 = tf.add(tf.matmul(tf.nn.dropout(W5, keep_prob),A4),b5)                       # Z5 = np.dot(W5,a4) + b5\n",
    "    A5 = tf.nn.relu(Z5)                                    # A5 = relu(Z5)\n",
    "    Z6 = tf.add(tf.matmul(W6,A5),b6)                       # Z6 = np.dot(W6,a5) + b6\n",
    "    A6 = tf.nn.relu(Z6)                                    # A5 = relu(Z5)\n",
    "    Z7 = tf.add(tf.matmul(W7,A6),b7)                       # Z6 = np.dot(W6,a5) + b6\n",
    "    A7 = tf.nn.relu(Z7)                                    # A5 = relu(Z5)\n",
    "    Z8 = tf.add(tf.matmul(tf.nn.dropout(W8, keep_prob),A7),b8)                       # Z6 = np.dot(W6,a5) + b6\n",
    "    A8 = tf.nn.relu(Z8)                                    # A5 = relu(Z5)\n",
    "    Z9 = tf.add(tf.matmul(W9,A8),b9)\n",
    "    A9 = tf.nn.relu(Z9)                                    # A5 = relu(Z5)\n",
    "    Z10 = tf.add(tf.matmul(W10,A9),b10)\n",
    "#     A10 = tf.nn.relu(Z10)                                    # A5 = relu(Z5)\n",
    "#     Z11 = tf.add(tf.matmul(W11,A10),b11)\n",
    "#     A11 = tf.nn.relu(Z11)                                    # A5 = relu(Z5)\n",
    "#     Z12 = tf.add(tf.matmul(W12,A11),b12)\n",
    "#     A12 = tf.nn.relu(Z12)                                    # A5 = relu(Z5)\n",
    "#     Z13 = tf.add(tf.matmul(W13,A12),b13)\n",
    "#     A13 = tf.nn.relu(Z13)                                    # A5 = relu(Z5)\n",
    "#     Z14 = tf.add(tf.matmul(W14,A13),b14)\n",
    "#     A14 = tf.nn.relu(Z14)                                    # A5 = relu(Z5)\n",
    "#     Z15 = tf.add(tf.matmul(W15,A14),b15)\n",
    "#     A15 = tf.nn.relu(Z15)                                    # A5 = relu(Z5)\n",
    "#     Z16 = tf.add(tf.matmul(W16,A15),b16)\n",
    "#     A16 = tf.nn.relu(Z16)                                    # A5 = relu(Z5)\n",
    "#     Z17 = tf.add(tf.matmul(W17,A16),b17)\n",
    "#     A17 = tf.nn.relu(Z17)                                    # A5 = relu(Z5)\n",
    "#     Z18 = tf.add(tf.matmul(W18,A17),b18)\n",
    "#     A18 = tf.nn.relu(Z18)                                    # A5 = relu(Z5)\n",
    "#     Z19 = tf.add(tf.matmul(W19,A18),b19)\n",
    "#     A19 = tf.nn.relu(Z19)\n",
    "#     Z20 = tf.add(tf.matmul(W20,A19),b20)\n",
    "#     A20 = tf.nn.relu(Z20)                                    # A5 = relu(Z5)\n",
    "#     Z21 = tf.add(tf.matmul(W21,A20),b21)\n",
    "#     A21 = tf.nn.relu(Z21)                                    # A5 = relu(Z5)\n",
    "#     Z22 = tf.add(tf.matmul(W22,A21),b22)\n",
    "#     A22 = tf.nn.relu(Z22)\n",
    "#     Z23 = tf.add(tf.matmul(W23,A22),b23)\n",
    "#     A23 = tf.nn.relu(Z23)                                    # A5 = relu(Z5)\n",
    "#     Z24 = tf.add(tf.matmul(W24,A23),b24)\n",
    "#     A24 = tf.nn.relu(Z24)                                    # A5 = relu(Z5)\n",
    "#     Z25 = tf.add(tf.matmul(W25,A24),b25)\n",
    "#     A25 = tf.nn.relu(Z25)                                    # A5 = relu(Z5)\n",
    "#     Z26 = tf.add(tf.matmul(W26,A25),b26)\n",
    "#     A26 = tf.nn.relu(Z26)                                    # A5 = relu(Z5)\n",
    "#     Z27 = tf.add(tf.matmul(W27,A26),b27)\n",
    "#     A27 = tf.nn.relu(Z27)                                    # A5 = relu(Z5)\n",
    "#     Z28 = tf.add(tf.matmul(W28,A27),b28)\n",
    "#     A28 = tf.nn.relu(Z28)                                    # A5 = relu(Z5)\n",
    "#     Z29 = tf.add(tf.matmul(W29,A28),b29)\n",
    "#     A29 = tf.nn.relu(Z29)                                    # A5 = relu(Z5)\n",
    "#     Z30 = tf.add(tf.matmul(W30,A29),b30)\n",
    "#     A30 = tf.nn.relu(Z30)                                    # A5 = relu(Z5)\n",
    "#     Z31 = tf.add(tf.matmul(W31,A30),b31)\n",
    "#     ### END CODE HERE ###\n",
    "    \n",
    "    return Z10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z12 = Tensor(\"Add_4:0\", shape=(6, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(93, 34)\n",
    "    parameters = initialize_parameters()\n",
    "    Z12 = forward_propagation(X, parameters)\n",
    "    print(\"Z12 = \" + str(Z12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost \n",
    "\n",
    "def compute_cost(Z31, Y, params):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z4 -- output of forward propagation (output of the last LINEAR unit), of shape (34, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z4\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z31)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    ### START CODE HERE ### (1 line of code)...\n",
    "    n = len(params)//2\n",
    "    regularizer = 0\n",
    "    for x in range(n):\n",
    "        regularizer = regularizer + tf.nn.l2_loss(params['W'+str(x+1)])\n",
    "    regularizer = tf.nn.l2_loss(params['W'+str(n)])\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = labels) + 0.9*regularizer)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-ff98a105d657>:20: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "cost = Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(93, 34)\n",
    "    parameters = initialize_parameters()\n",
    "    Z31 = forward_propagation(X, parameters)\n",
    "    cost = compute_cost(Z31, Y)\n",
    "    print(\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: random_mini_batches\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size = 512, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    k = 0\n",
    "        \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X.iloc[:, permutation]\n",
    "    shuffled_Y = Y.iloc[:, permutation]\n",
    "#.reshape((1,m))\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X.iloc[:, k*mini_batch_size : (k+1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y.iloc[:, k*mini_batch_size : (k+1) * mini_batch_size]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X.iloc[:, (k+1)*mini_batch_size :  ]\n",
    "        mini_batch_Y = shuffled_Y.iloc[:, (k+1)*mini_batch_size :  ]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93, 15998)\n",
      "(6, 15998)\n"
     ]
    }
   ],
   "source": [
    "# X_train = X_train.T\n",
    "# Y_train = Y_train.T\n",
    "# X_test  = X_test.T\n",
    "# Y_test  = Y_test.T\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 1500, minibatch_size = 1024, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a four-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (input size = 93, number of training examples = 16129)\n",
    "    Y_train -- test set, of shape (output size = 34, number of training examples = 16129)\n",
    "    X_test -- training set, of shape (input size = 93, number of training examples = 3871)\n",
    "    Y_test -- test set, of shape (output size = 34, number of test examples = 3871)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(0)                             # to keep consistent results\n",
    "    seed = 2                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    X, Y = tf.placeholder(dtype = \"float32\", shape=(n_x, None) , name=\"X\"), tf.placeholder(dtype = \"float32\", shape=(n_y,None) , name=\"Y\")\n",
    "    keep_prob = tf.placeholder(\"float\")\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Initialize parameters\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    parameters = initialize_parameters()\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    #print(\"here \",X.shape)\n",
    "    Z20 = forward_propagation(X, parameters)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    cost = compute_cost(Z20, Y, parameters)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                ### START CODE HERE ### (1 line)\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y, keep_prob : 0.5})\n",
    "\n",
    "                ### END CODE HERE ###\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z20), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    \n",
    "        print (\"Train Accuracy : \", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy  : \", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        print (\"Learning_rate  : \",learning_rate)\n",
    "        print (\"Batch Size     : \",minibatch_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93, 15998)\n",
      "Cost after epoch 0: 5.103737\n",
      "Cost after epoch 100: 1.798817\n",
      "Cost after epoch 200: 1.781979\n",
      "Cost after epoch 300: 1.764260\n",
      "Cost after epoch 400: 1.758724\n",
      "Cost after epoch 500: 1.743914\n",
      "Cost after epoch 600: 1.746017\n",
      "Cost after epoch 700: 1.748922\n",
      "Cost after epoch 800: 1.735863\n",
      "Cost after epoch 900: 1.724981\n",
      "Cost after epoch 1000: 1.736197\n",
      "Cost after epoch 1100: 1.740117\n",
      "Cost after epoch 1200: 1.728667\n",
      "Cost after epoch 1300: 1.731138\n",
      "Cost after epoch 1400: 1.732938\n",
      "Cost after epoch 1500: 1.727581\n",
      "Cost after epoch 1600: 1.721879\n",
      "Cost after epoch 1700: 1.724845\n",
      "Cost after epoch 1800: 1.734480\n",
      "Cost after epoch 1900: 1.733022\n",
      "Cost after epoch 2000: 1.722924\n",
      "Cost after epoch 2100: 1.737421\n",
      "Cost after epoch 2200: 1.726776\n",
      "Cost after epoch 2300: 1.726131\n",
      "Cost after epoch 2400: 1.721453\n",
      "Cost after epoch 2500: 1.718787\n",
      "Cost after epoch 2600: 1.731780\n",
      "Cost after epoch 2700: 1.717676\n",
      "Cost after epoch 2800: 1.730933\n",
      "Cost after epoch 2900: 1.720377\n",
      "Cost after epoch 3000: 1.730743\n",
      "Cost after epoch 3100: 1.729842\n",
      "Cost after epoch 3200: 1.723388\n",
      "Cost after epoch 3300: 1.724694\n",
      "Cost after epoch 3400: 1.734065\n",
      "Cost after epoch 3500: 1.720524\n",
      "Cost after epoch 3600: 1.719783\n",
      "Cost after epoch 3700: 1.729446\n",
      "Cost after epoch 3800: 1.726661\n",
      "Cost after epoch 3900: 1.721839\n",
      "Cost after epoch 4000: 1.730209\n",
      "Cost after epoch 4100: 1.729690\n",
      "Cost after epoch 4200: 1.728493\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n",
      "Train Accuracy :  0.29022378\n",
      "Test Accuracy  :  0.18665667\n",
      "Learning_rate  :  0.005\n",
      "Batch Size     :  1000\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape) \n",
    "parameters= model(X_train, Y_train, X_test, Y_test,num_epochs = 4201,learning_rate = 0.005, minibatch_size = 1000)\n",
    "#24,20,2.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(label, confusion_matrix):\n",
    "    col = confusion_matrix[:, label]\n",
    "    return confusion_matrix[label, label] / col.sum()\n",
    "# 0.41639057\n",
    "# Test Accuracy  :  0.1678741\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(label, confusion_matrix):\n",
    "    row = confusion_matrix[label, :]\n",
    "    return confusion_matrix[label, label] / row.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_macro_average(confusion_matrix):\n",
    "    rows, columns = confusion_matrix.shape\n",
    "    sum_of_precisions = 0\n",
    "    for label in range(rows):\n",
    "        sum_of_precisions += precision(label, confusion_matrix)\n",
    "    return sum_of_precisions / rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_macro_average(confusion_matrix):\n",
    "    rows, columns = confusion_matrix.shape\n",
    "    sum_of_recalls = 0\n",
    "    for label in range(columns):\n",
    "        sum_of_recalls += recall(label, confusion_matrix)\n",
    "    return sum_of_recalls / columns\n",
    "\n",
    "# y_Actual = \n",
    "# df = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\n",
    "# confusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "# sn.heatmap(confusion_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
